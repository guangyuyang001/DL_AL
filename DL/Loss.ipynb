{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8487f297",
   "metadata": {},
   "source": [
    "# 损失函数（Loss Function）和成本函数（Cost Function）\n",
    "    \n",
    "   损失函数用于单个训练样本，有时也称误差函数；成本函数是整个数据集的平均损失。  \n",
    "   **优化策略** 旨在最小化成本函数。  \n",
    "   损失函数分类：\n",
    "   1. 基于距离度量的损失函数  回归问题  \n",
    "   通常将输入数据映射到基于距离的特征空间上，如欧氏空间、汉明空间等，将映射后的样本看作空间上的点，采用合适的损失函数度量特征空间上样本真实值和模型预测值之间的距离。特征空间上两个点的距离越小，模型预测性能越好。   \n",
    "     1. 均方误差损失函数\n",
    "     2. L2损失函数\n",
    "     3. L1损失函数\n",
    "     4. Smooth L1损失函数\n",
    "     4. huber损失函数\n",
    "   2. 基于概率分布度量的损失函数 分类问题  \n",
    "   将样本间的相似性转化为随机事件出现的可能性，即通过度量样本的真实分布与预测分布之间的距离判断两者的相似度，一般用于涉及概率分布或预测类别出现的概率应用问题中，在分类问题中尤为常用\n",
    "     1. KL散度（相对熵）\n",
    "     2. 交叉熵损失\n",
    "     3. sofmax损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db0b97",
   "metadata": {},
   "source": [
    "# 均方差误差(Mean Square Error, MSE) \n",
    "通过计算预测值与真实值之间差异的平方和来衡量模型性能和模型预测结果与真实值之间的差异程度。   \n",
    "计算公式：\n",
    "          $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\tag{1}$$ \n",
    "其中：$n$是样本数量，$y_i$是第$i$个样本的真实值，$\\hat{y}_i$是第$i$个样本的预测值。    \n",
    "优点：MSE的函数曲线光滑、连续、处处可导，便于使用梯度下降算法，随着误差的减小，梯度也在减小有利于收敛。  \n",
    "缺点：当真实值与预测值差值大于1时会放大误差，差值小于1时会缩小误差，对离群点比较敏感，受影响较大。   \n",
    "MSE损失函数对异常值的健壮性较低，如果数据有多个异常值其与真值的差异会较大，因此MSE会更大导致网络训练不稳定。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a5887a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.176\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 真实值和预测值\n",
    "y_true = np.array([1, 2, 3, 4, 5])\n",
    "y_pred = np.array([1.2, 2.5, 3.7, 4.1, 5.3])\n",
    "\n",
    "# 计算均方误差\n",
    "mse = np.mean((y_true - y_pred) ** 2)\n",
    "print(\"MSE: \", mse)\n",
    "\n",
    "# 计算梯度\n",
    "def update_weight_MSE(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "        # 计算偏导数\n",
    "        m_deriv += -2*X[i]*(Y[i] - (m*X[i] + b))\n",
    "        b_deriv += -2*(Y[i] - (m*X[i] + b))\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "    \n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e7a24",
   "metadata": {},
   "source": [
    "# 平均绝对误差(Mean Absolute Error, MAE)\n",
    "衡量模型的预测值与实际值之间的平均绝对差异，预测模型性能的损失函数，通常用于回归问题。  \n",
    "   $MAE = \\frac{1}{n} \\sum_{i=1}^{n}\\left | y_i - \\hat{y}_i \\right | $  \n",
    "其中：MAE是平均绝对误差，$n$是样本数量，$y_i$是第$i$个样本的真实值，$\\hat{y}_i$是第$i$个样本的预测值。\n",
    "\n",
    "与MSE相比对异常值的健壮性更强但是在数学方程中处理绝对值或模数运算并不容易。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af63bdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.4644732384427797\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成随机数据\n",
    "np.random.seed(0)\n",
    "n = 50\n",
    "X = np.linspace(0, 10, n)\n",
    "y_true = 2 * X + np.random.normal(0, 1, n) # 真实的目标值，包含随机噪音\n",
    "y_pred = 2 * X + 1.5\n",
    "\n",
    "# 计算MAE\n",
    "mae = np.mean(np.abs(y_true - y_pred))\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daaabd9",
   "metadata": {},
   "source": [
    "# 交叉熵损失（Cross-Entropy Loss)\n",
    "\n",
    "交叉熵损失基于信息论中的交叉熵概念，衡量模型预测结果与实际标签之间的差异，度量两个概率分布之间的相似性。在分类问题中，将实际标签表示为one-hot向量，即只有一个元素为1，其余元素为0；而模型预测结果通常是一个概率分布。   \n",
    "$$Loss = -\\sum_{i=1}^{N} y_i log(p_i) $$   \n",
    "其中，y_i为实际标签中第$i$个类别的概率（one-hot编码），$p_i$为模型预测的第$i$个类别的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9180b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cross_entropy_loss(y_true, p_pred):\n",
    "    epsilon = 1e-10\n",
    "    return -np.sum(y_true*np.log(p_pred + epsilon), axis = 1)\n",
    "\n",
    "num_samples = 1000\n",
    "num_classes = 5\n",
    "np.random.seed(42)\n",
    "\n",
    "y_true = np.eye(num_classes)[np.random.choice(num_classes, num_samples)]\n",
    "p_pred = np.random.rand(num_samples, num_classes)\n",
    "\n",
    "loss = cross_entropy_loss(y_true, p_pred)\n",
    "average = np.mean(loss)\n",
    "# print('Loss：', loss, 'average：', average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af1afe",
   "metadata": {},
   "source": [
    "# 对数损失（Log Loss）\n",
    "对数损失也称二分类交叉损失，衡量两个概率分布之间的相似性，对数损失通过计算实际标签和模型预测结果的交叉熵来评估模型性能的。  \n",
    "公式：  \n",
    "$$Loss = -(ylog(p) + (1 - y)log(1 - p))$$  \n",
    "其中，$y$为实际标签，$p$为模型预测的概率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4df29d",
   "metadata": {},
   "source": [
    "余弦相似度损失\n",
    "用来衡量两个向量之间的相似度，基于余弦相似度的概念。余弦相似度是两个向量之间夹角的余弦值，范围在-1到1之间，当夹角为0度时，两个向量完全相同；当夹角为90度时，余弦相似度为0，表示两个向量无关；当夹角为180度时，余弦相似度为-1，表示两个向量完全相反。\n",
    "\n",
    "$$Loss = \\frac{A·B}{\\left \\| A \\right \\| \\left \\| B \\right \\|}$$\n",
    "\n",
    "其中，$A·B$表示向量$A$和向量$B$的内积，$\\left \\| A \\right \\|$和$\\left \\| B \\right \\|$向量$A$和向量$B$的范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca146f66",
   "metadata": {},
   "source": [
    "# 希尔伯特-施密特口袋\n",
    "是一种支持向量机损失函数，旨在更好处理线性可分和不可分的问题。主要思想是要找到一个超平面，将两个类别的数据分开，并最大化这个超平面与最近的数据点（支持向量）之间的间隔。  \n",
    "公式：  \n",
    "$$L(w, b)=\\frac{1}{2}\\|w\\|^{2}+C \\sum_{i=1}^{N} \\max \\left(0,1-y_{i}\\left(w \\cdot x_{i}+b\\right)\\right)$$\n",
    "\n",
    "其中，$w$是超平面的法向量，$b$是偏置，$x_{i}$是训练样本，$y_{i}$是对应的标签（+1或-1），N是样本数量；第一部分是最小化样本权重向量$w$的大小，即最大化间隔，防止过拟合；第二部分是最小化误分类样本数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc3fca",
   "metadata": {},
   "source": [
    "# Huber\n",
    "是一种用于回归问题的损失函数，它对异常值不敏感，主要思想是在距离真实值较近的情况下采用均方误差，而在距离较远的情况下采用绝对误差。\n",
    "$$ \n",
    "\\begin{align}\n",
    "L(y, \\hat{y}) & = \\left\\{\\begin{array}{ll}\n",
    "\\frac{1}{2}(y-\\hat{y})^{2}, & \\text { 如果 }|y-\\hat{y}| \\leq \\delta \\\\\n",
    "\\delta|y-\\hat{y}|-\\frac{1}{2} \\delta^{2}, & \\text { 如果 }|y-\\hat{y}|>\\delta\n",
    "\\end{array}\\right.\n",
    "\\end{align}\n",
    "$$  \n",
    "其中，$\\delta$是一个超参数用于控制在何种情况下切换使用均方误差和绝对误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a31f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, delta):\n",
    "    error = np.abs(y_true - y_pred)\n",
    "    if error <= delta:\n",
    "        return 0.5 * error**2\n",
    "    else:\n",
    "        return delta * error - 0.5 * delta**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
